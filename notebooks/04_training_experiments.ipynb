{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd710d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba874ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb423c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"accelerate>=0.26.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41584a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a995961c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ca88492",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f149cde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: rows=8309 | empty_en=0 | empty_ig=0 | dup=0 | starts_quote_en=0 | starts_quote_ig=0\n",
      "  OK — no obvious problems found.\n",
      "validation: rows=1068 | empty_en=0 | empty_ig=0 | dup=0 | starts_quote_en=0 | starts_quote_ig=0\n",
      "  OK — no obvious problems found.\n",
      "test: rows=1069 | empty_en=0 | empty_ig=0 | dup=0 | starts_quote_en=0 | starts_quote_ig=0\n",
      "  OK — no obvious problems found.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files={\n",
    "    \"train\": \"../Dataset/processed/train.csv\",\n",
    "    \"validation\": \"../Dataset/processed/val.csv\",\n",
    "    \"test\": \"../Dataset/processed/test.csv\"\n",
    "})\n",
    "\n",
    "def normalize_types(batch):\n",
    "    # ensure values are strings and trimmed (fast, safe)\n",
    "    batch[\"english\"] = [str(x).strip() if x is not None else \"\" for x in batch.get(\"english\", [])]\n",
    "    batch[\"igbo\"]    = [str(x).strip() if x is not None else \"\" for x in batch.get(\"igbo\", [])]\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(normalize_types, batched=True, desc=\"normalize types and trim\")\n",
    "\n",
    "# Quick checks & summary\n",
    "for split in [\"train\", \"validation\", \"test\"]:\n",
    "    ds = dataset[split]\n",
    "    df = ds.to_pandas()\n",
    "    total = len(df)\n",
    "    empty_en = df[\"english\"].eq(\"\").sum()\n",
    "    empty_ig = df[\"igbo\"].eq(\"\").sum()\n",
    "    dup = df.duplicated(subset=[\"english\",\"igbo\"]).sum()\n",
    "    starts_with_quote_en = df[\"english\"].str.startswith('\"').sum()\n",
    "    starts_with_quote_ig = df[\"igbo\"].str.startswith('\"').sum()\n",
    "    print(f\"{split}: rows={total} | empty_en={empty_en} | empty_ig={empty_ig} | dup={dup} | starts_quote_en={starts_with_quote_en} | starts_quote_ig={starts_with_quote_ig}\")\n",
    "\n",
    "    if empty_en or empty_ig or dup or starts_with_quote_en or starts_with_quote_ig:\n",
    "        print(\"  Sample problematic rows:\")\n",
    "        bad = df[(df[\"english\"]== \"\") | (df[\"igbo\"]==\"\") | (df.duplicated(subset=['english','igbo'], keep=False)) | (df[\"english\"].str.startswith('\"')) | (df[\"igbo\"].str.startswith('\"'))]\n",
    "        display(bad.head(10))\n",
    "    else:\n",
    "        print(\"  OK — no obvious problems found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1637b90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "train_df = pd.read_csv(\"../Dataset/processed/train_tokenized_hf.csv\")\n",
    "val_df   = pd.read_csv(\"../Dataset/processed/validation_tokenized_hf.csv\")\n",
    "\n",
    "# drop accidental index column from pandas if present\n",
    "for df in (train_df, val_df):\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'validation': Dataset.from_pandas(val_df),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88d6b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer_en = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"../tokenizers/english_tokenizer.json\",   \n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")\n",
    "\n",
    "tokenizer_ig = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"../tokenizers/igbo_tokenizer.json\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    mask_token=\"[MASK]\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83af5b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking tokenizer: English\n",
      "[UNK]: ID = 0\n",
      "[PAD]: ID = 1\n",
      "[CLS]: ID = 2\n",
      "[SEP]: ID = 3\n",
      "[MASK]: ID = 4\n",
      "\n",
      "Checking tokenizer: Igbo\n",
      "[UNK]: ID = 0\n",
      "[PAD]: ID = 1\n",
      "[CLS]: ID = 2\n",
      "[SEP]: ID = 3\n",
      "[MASK]: ID = 4\n"
     ]
    }
   ],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "\n",
    "def check_special_tokens(tokenizer, name):\n",
    "    print(f\"\\nChecking tokenizer: {name}\")\n",
    "    for token in special_tokens:\n",
    "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "        if token_id == tokenizer.unk_token_id and token != tokenizer.unk_token:\n",
    "            # Means token wasn't found, showing -1 (missing)\n",
    "            print(f\"{token}: MISSING in vocab\")\n",
    "        else:\n",
    "            print(f\"{token}: ID = {token_id}\")\n",
    "\n",
    "check_special_tokens(tokenizer_en, \"English\")\n",
    "check_special_tokens(tokenizer_ig, \"Igbo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "018234d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully!\n",
      "Example tokens: ['▁Hello', '▁world', '!']\n",
      "Token IDs: [256047, 94124, 15697, 248203, 2]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"facebook/nllb-200-distilled-600M\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "print(\"Tokenizer loaded successfully!\")\n",
    "\n",
    "# Test tokenization\n",
    "sample_text = \"Hello world!\"\n",
    "tokens = tokenizer.tokenize(sample_text)\n",
    "print(\"Example tokens:\", tokens)\n",
    "\n",
    "# If you want token IDs:\n",
    "token_ids = tokenizer.encode(sample_text, add_special_tokens=True)\n",
    "print(\"Token IDs:\", token_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f886e2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ScaledWordEmbedding(256204, 1024, padding_idx=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b73d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SOURCE_LENGTH = 128\n",
    "MAX_TARGET_LENGTH = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "476a4653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c25b54089043de9914475dafc39ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and formatting datasets:   0%|          | 0/8309 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c765a604bbb4853b0c03cebe76b56fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing and formatting datasets:   0%|          | 0/1068 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(batch):\n",
    "    src_texts = [str(x) if x is not None else \"\" for x in batch['english']]\n",
    "    tgt_texts = [str(x) if x is not None else \"\" for x in batch['igbo']]\n",
    "\n",
    "    inputs = tokenizer_en(\n",
    "        src_texts,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=MAX_SOURCE_LENGTH,\n",
    "        return_attention_mask=True\n",
    "    )\n",
    "\n",
    "    labels = tokenizer_ig(\n",
    "        tgt_texts,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        max_length=MAX_TARGET_LENGTH\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'labels': labels['input_ids']\n",
    "    }\n",
    "\n",
    "cols_to_remove = [c for c in ['english','igbo','english_tokens','igbo_tokens'] if c in dataset['train'].column_names]\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=cols_to_remove,\n",
    "    desc=\"Tokenizing and formatting datasets\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c312bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English (decoded): why did you leave your former place of work ?\n",
      "Igbo (decoded): gịnị mere i ji hapụ ebe ị na - arụ n ' oge mbu ?\n"
     ]
    }
   ],
   "source": [
    "sample = tokenized['train'][0]\n",
    "print(\"English (decoded):\", tokenizer_en.decode(sample['input_ids'], skip_special_tokens=True))\n",
    "print(\"Igbo (decoded):\", tokenizer_ig.decode(sample['labels'], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf935718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version: 4.55.0\n",
      "transformers file: /Users/bigdreams/Documents/langGpt/venv/lib/python3.13/site-packages/transformers/__init__.py\n",
      "Seq2SeqTrainingArguments module: transformers.training_args_seq2seq\n",
      "Seq2SeqTrainingArguments file: /Users/bigdreams/Documents/langGpt/venv/lib/python3.13/site-packages/transformers/training_args_seq2seq.py\n",
      "Seq2SeqTrainingArguments __init__ signature:\n",
      "(self, output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: Optional[float] = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict[str, Any], str, NoneType] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: str = 'passive', log_level_replica: str = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: Optional[bool] = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, use_ipex: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: Optional[bool] = True, label_names: Optional[list[str]] = None, load_best_model_at_end: Optional[bool] = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = '', fsdp_min_num_params: int = 0, fsdp_config: Union[dict[str, Any], str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: Optional[str] = 'length', report_to: Union[NoneType, str, list[str]] = None, ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, hub_revision: Optional[str] = None, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict[str, Any], str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: int = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: Optional[bool] = False, include_num_input_tokens_seen: Optional[bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: Optional[bool] = False, liger_kernel_config: Optional[dict[str, bool]] = None, eval_use_gather_object: Optional[bool] = False, average_tokens_across_devices: Optional[bool] = True, sortish_sampler: bool = False, predict_with_generate: bool = False, generation_max_length: Optional[int] = None, generation_num_beams: Optional[int] = None, generation_config: Union[str, pathlib._local.Path, transformers.generation.configuration_utils.GenerationConfig, NoneType] = None) -> None\n"
     ]
    }
   ],
   "source": [
    "import transformers, inspect\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "print(\"transformers version:\", transformers.__version__)\n",
    "print(\"transformers file:\", transformers.__file__)\n",
    "print(\"Seq2SeqTrainingArguments module:\", Seq2SeqTrainingArguments.__module__)\n",
    "print(\"Seq2SeqTrainingArguments file:\", inspect.getsourcefile(Seq2SeqTrainingArguments))\n",
    "print(\"Seq2SeqTrainingArguments __init__ signature:\")\n",
    "import inspect\n",
    "print(inspect.signature(Seq2SeqTrainingArguments.__init__))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7564ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# Training Arguments\n",
    "train_args = Seq2SeqTrainingArguments(\n",
    "    output_dir='./results',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=15,\n",
    "    learning_rate=3e-5,\n",
    "    logging_dir='./logs',\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_loss',\n",
    "    greater_is_better=False,\n",
    "    max_steps=5 \n",
    "    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b7f01f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "M2M100ForConditionalGeneration(\n",
       "  (model): M2M100Model(\n",
       "    (shared): M2M100ScaledWordEmbedding(256204, 1024, padding_idx=1)\n",
       "    (encoder): M2M100Encoder(\n",
       "      (embed_tokens): M2M100ScaledWordEmbedding(256204, 1024, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x M2M100EncoderLayer(\n",
       "          (self_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): ReLU()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): M2M100Decoder(\n",
       "      (embed_tokens): M2M100ScaledWordEmbedding(256204, 1024, padding_idx=1)\n",
       "      (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x M2M100DecoderLayer(\n",
       "          (self_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): ReLU()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): M2M100Attention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=256204, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18018812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FP16 enabled: False\n"
     ]
    }
   ],
   "source": [
    "print(\"FP16 enabled:\", train_args.fp16)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,           \n",
    "    args=train_args,\n",
    "    train_dataset=tokenized['train'],\n",
    "    eval_dataset=tokenized['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fb0ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
